{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "## *Data Science Unit 4 Sprint 3 Assignment 1*\n",
    "\n",
    "# Recurrent Neural Networks and Long Short Term Memory (LSTM)\n",
    "\n",
    "![Monkey at a typewriter](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3c/Chimpanzee_seated_at_typewriter.jpg/603px-Chimpanzee_seated_at_typewriter.jpg)\n",
    "\n",
    "It is said that [infinite monkeys typing for an infinite amount of time](https://en.wikipedia.org/wiki/Infinite_monkey_theorem) will eventually type, among other things, the complete works of Wiliam Shakespeare. Let's see if we can get there a bit faster, with the power of Recurrent Neural Networks and LSTM.\n",
    "\n",
    "This text file contains the complete works of Shakespeare: https://www.gutenberg.org/files/100/100-0.txt\n",
    "\n",
    "Use it as training data for an RNN - you can keep it simple and train character level, and that is suggested as an initial approach.\n",
    "\n",
    "Then, use that trained RNN to generate Shakespearean-ish text. Your goal - a function that can take, as an argument, the size of text (e.g. number of characters or lines) to generate, and returns generated text of that size.\n",
    "\n",
    "Note - Shakespeare wrote an awful lot. It's OK, especially initially, to sample/use smaller data and parameters, so you can have a tighter feedback loop when you're trying to get things running. Then, once you've got a proof of concept - start pushing it more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-06-15T18:18:20.453Z",
     "iopub.status.busy": "2020-06-15T18:18:20.442Z",
     "iopub.status.idle": "2020-06-15T18:18:20.513Z",
     "shell.execute_reply": "2020-06-15T18:18:20.523Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-06-15T18:25:49.781Z",
     "iopub.status.busy": "2020-06-15T18:25:49.778Z",
     "iopub.status.idle": "2020-06-15T18:25:51.467Z",
     "shell.execute_reply": "2020-06-15T18:25:51.469Z"
    }
   },
   "outputs": [],
   "source": [
    "url = \"https://www.gutenberg.org/files/100/100-0.txt\"\n",
    "\n",
    "r = requests.get(url)\n",
    "r.encoding = r.apparent_encoding\n",
    "data = r.text\n",
    "data = data.split('\\r\\n')\n",
    "toc = [l.strip() for l in data[44:130:2]]\n",
    "# Skip the Table of Contents\n",
    "data = data[135:]\n",
    "\n",
    "# Fixing Titles\n",
    "toc[9] = 'THE LIFE OF KING HENRY V'\n",
    "toc[18] = 'MACBETH'\n",
    "toc[24] = 'OTHELLO, THE MOOR OF VENICE'\n",
    "toc[34] = 'TWELFTH NIGHT: OR, WHAT YOU WILL'\n",
    "\n",
    "locations = {id_:{'title':title, 'start':-99} for id_,title in enumerate(toc)}\n",
    "\n",
    "# Start \n",
    "for e,i in enumerate(data):\n",
    "    for t,title in enumerate(toc):\n",
    "        if title in i:\n",
    "            locations[t].update({'start':e})\n",
    "            \n",
    "\n",
    "df_toc = pd.DataFrame.from_dict(locations, orient='index')\n",
    "df_toc['end'] = df_toc['start'].shift(-1).apply(lambda x: x-1)\n",
    "df_toc.loc[42, 'end'] = len(data)\n",
    "df_toc['end'] = df_toc['end'].astype('int')\n",
    "\n",
    "df_toc['text'] = df_toc.apply(lambda x: '\\r\\n'.join(data[ x['start'] : int(x['end']) ]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-06-15T18:26:12.637Z",
     "iopub.status.busy": "2020-06-15T18:26:12.630Z",
     "iopub.status.idle": "2020-06-15T18:26:12.643Z",
     "shell.execute_reply": "2020-06-15T18:26:12.647Z"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ALL’S WELL THAT ENDS WELL</td>\n",
       "      <td>2777</td>\n",
       "      <td>7738</td>\n",
       "      <td>ALL’S WELL THAT ENDS WELL\\r\\n\\r\\n\\r\\n\\r\\nConte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>THE TRAGEDY OF ANTONY AND CLEOPATRA</td>\n",
       "      <td>7739</td>\n",
       "      <td>11840</td>\n",
       "      <td>THE TRAGEDY OF ANTONY AND CLEOPATRA\\r\\n\\r\\nDRA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AS YOU LIKE IT</td>\n",
       "      <td>11841</td>\n",
       "      <td>14631</td>\n",
       "      <td>AS YOU LIKE IT\\r\\n\\r\\nDRAMATIS PERSONAE.\\r\\n\\r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>THE COMEDY OF ERRORS</td>\n",
       "      <td>14632</td>\n",
       "      <td>17832</td>\n",
       "      <td>THE COMEDY OF ERRORS\\r\\n\\r\\n\\r\\n\\r\\nContents\\r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>THE TRAGEDY OF CORIOLANUS</td>\n",
       "      <td>17833</td>\n",
       "      <td>27806</td>\n",
       "      <td>THE TRAGEDY OF CORIOLANUS\\r\\n\\r\\nDramatis Pers...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 title  start    end  \\\n",
       "0            ALL’S WELL THAT ENDS WELL   2777   7738   \n",
       "1  THE TRAGEDY OF ANTONY AND CLEOPATRA   7739  11840   \n",
       "2                       AS YOU LIKE IT  11841  14631   \n",
       "3                 THE COMEDY OF ERRORS  14632  17832   \n",
       "4            THE TRAGEDY OF CORIOLANUS  17833  27806   \n",
       "\n",
       "                                                text  \n",
       "0  ALL’S WELL THAT ENDS WELL\\r\\n\\r\\n\\r\\n\\r\\nConte...  \n",
       "1  THE TRAGEDY OF ANTONY AND CLEOPATRA\\r\\n\\r\\nDRA...  \n",
       "2  AS YOU LIKE IT\\r\\n\\r\\nDRAMATIS PERSONAE.\\r\\n\\r...  \n",
       "3  THE COMEDY OF ERRORS\\r\\n\\r\\n\\r\\n\\r\\nContents\\r...  \n",
       "4  THE TRAGEDY OF CORIOLANUS\\r\\n\\r\\nDramatis Pers...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Shakespeare Data Parsed by Play\n",
    "df_toc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for LSTM\n",
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our corpus contains 106 unique characters.\n"
     ]
    }
   ],
   "source": [
    "# Encode Data as Chars\n",
    "\n",
    "# Gather all text \n",
    "# Why? 1. See all possible characters 2. For training / splitting later\n",
    "text = \" \".join(df_toc['text'])\n",
    "\n",
    "# Unique Characters\n",
    "chars = list(set(text))\n",
    "\n",
    "# Lookup Tables\n",
    "char_int = {c:i for i, c in enumerate(chars)} \n",
    "int_char = {i:c for i, c in enumerate(chars)} \n",
    "print(f\"Our corpus contains {len(chars)} unique characters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequences:  281791\n"
     ]
    }
   ],
   "source": [
    "# Create the sequence data\n",
    "\n",
    "maxlen = 40\n",
    "step = 20\n",
    "\n",
    "encoded = [char_int[c] for c in text]\n",
    "\n",
    "sequences = [] # Each element is 40 chars long\n",
    "next_char = [] # One element for each sequence\n",
    "\n",
    "for i in range(0, len(encoded) - maxlen, step):\n",
    "    \n",
    "    sequences.append(encoded[i : i + maxlen])\n",
    "    next_char.append(encoded[i + maxlen])\n",
    "    \n",
    "print('sequences: ', len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[51,\n",
       " 83,\n",
       " 83,\n",
       " 84,\n",
       " 103,\n",
       " 43,\n",
       " 11,\n",
       " 44,\n",
       " 83,\n",
       " 83,\n",
       " 43,\n",
       " 41,\n",
       " 82,\n",
       " 51,\n",
       " 41,\n",
       " 43,\n",
       " 44,\n",
       " 25,\n",
       " 73,\n",
       " 103,\n",
       " 43,\n",
       " 11,\n",
       " 44,\n",
       " 83,\n",
       " 83,\n",
       " 60,\n",
       " 63,\n",
       " 60,\n",
       " 63,\n",
       " 60,\n",
       " 63,\n",
       " 60,\n",
       " 63,\n",
       " 71,\n",
       " 35,\n",
       " 20,\n",
       " 45,\n",
       " 30,\n",
       " 20,\n",
       " 45]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(281791, 40, 106)\n",
      "(281791, 106)\n"
     ]
    }
   ],
   "source": [
    "# Create x & y\n",
    "\n",
    "x = np.zeros((len(sequences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sequences),len(chars)), dtype=np.bool)\n",
    "\n",
    "for i, sequence in enumerate(sequences):\n",
    "    for t, char in enumerate(sequence):\n",
    "        x[i,t,char] = 1\n",
    "        \n",
    "    y[i, next_char[i]] = 1\n",
    "        \n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model: a single LSTM\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(chars)), dropout=0.2))\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 128)               120320    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 106)               13674     \n",
      "=================================================================\n",
      "Total params: 133,994\n",
      "Trainable params: 133,994\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / 1\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_epoch_end(epoch, _):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    \n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "    \n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    \n",
    "    generated = ''\n",
    "    \n",
    "    sentence = text[start_index: start_index + maxlen]\n",
    "    generated += sentence\n",
    "    \n",
    "    print('----- Generating with seed: \"' + sentence + '\"')\n",
    "    sys.stdout.write(generated)\n",
    "    \n",
    "    for i in range(400):\n",
    "        x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x_pred[0, t, char_int[char]] = 1\n",
    "            \n",
    "        preds = model.predict(x_pred, verbose=0)[0]\n",
    "        next_index = sample(preds)\n",
    "        next_char = int_char[next_index]\n",
    "        \n",
    "        sentence = sentence[1:] + next_char\n",
    "        \n",
    "        sys.stdout.write(next_char)\n",
    "        sys.stdout.flush()\n",
    "    print()\n",
    "\n",
    "\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "8804/8806 [============================>.] - ETA: 0s - loss: 2.4457\n",
      "----- Generating text after Epoch: 0\n",
      "----- Generating with seed: \"t of wreak in thee, that wilt revenge\n",
      " \"\n",
      "t of wreak in thee, that wilt revenge\n",
      "    Lime shatliro, bute ’ tasessing thin\n",
      "    My hom’se lo hod sard yor eyercense colk,\n",
      "    That Igain far ffale op'swanit? of wits.\n",
      "\n",
      "D\n",
      "                 As ''d   perestont.\n",
      "\n",
      ".ESIUSYN\n",
      "    in makn aith cancie and, tertiom hay thers.\n",
      " WUSEUN.\n",
      "The dench pakivef, Gofricke the cap dis?\n",
      "  POSGY\n",
      "'nsprasray'd thyn pare-s thay fore sheot mete co ar an lomp.n\n",
      "    He thy lingow a dith my hes thas mI\n",
      "8806/8806 [==============================] - 151s 17ms/step - loss: 2.4457\n",
      "Epoch 2/10\n",
      "8806/8806 [==============================] - ETA: 0s - loss: 2.1644\n",
      "----- Generating text after Epoch: 1\n",
      "----- Generating with seed: \"iter forbid,\n",
      "And say in thunder ‘Achill\"\n",
      "iter forbid,\n",
      "And say in thunder ‘Achillid?\n",
      "\n",
      "FONA.\n",
      "Mase stris\n",
      "    What nuthearianids mhples, spols nesmi; and cempte chongs wall notem.\n",
      "    Bo inghatyng iss: nad dy platy un bound_\n",
      "    Thot sope to lopl ligef bo noold drie the\n",
      "    Thathepl oo knob suloon 'ton to swrineck not dove;\n",
      "Whot hous. Whill' moy ho shalle las!\n",
      "If and thiuss, moupthy upase, Markn forod; inss wotthe.\n",
      "Woke fonkey, all oucr you brits and mung.\n",
      "  DAmUTHEt. \n",
      "8806/8806 [==============================] - 148s 17ms/step - loss: 2.1644\n",
      "Epoch 3/10\n",
      "8805/8806 [============================>.] - ETA: 0s - loss: 2.0726\n",
      "----- Generating text after Epoch: 2\n",
      "----- Generating with seed: \"and to the state of my great grief,\n",
      "   \"\n",
      "and to the state of my great grief,\n",
      "    My this to in his a lose aspue see to bruked\n",
      "    Tsatod, hay some bucur in retmon hast?\n",
      "  CONERVO. Alonelt in ot the Vavactina.\n",
      "\n",
      "OCO. SICICHOLIA.\n",
      "Cames, sumape, me what a way ont. Then threame,\n",
      "intyou purusanfen shros I hear wasting;\n",
      "\n",
      "  MIEGLOT.\n",
      "    to hty dooe urees ave lidd hassld on thuy ofor in.\n",
      "    Aff thiu burt me uple the murt? Hor'd.t\n",
      "    Muk' ip it. KAne alspwer spush tike thou\n",
      "8806/8806 [==============================] - 149s 17ms/step - loss: 2.0726\n",
      "Epoch 4/10\n",
      "8805/8806 [============================>.] - ETA: 0s - loss: 2.0149\n",
      "----- Generating text after Epoch: 3\n",
      "----- Generating with seed: \"g.\n",
      "    But this is mere digression from\"\n",
      "g.\n",
      "    But this is mere digression from with Morl man you an\n",
      "    I smoture ; sealoblewsowstants, you the breting,\n",
      "Their leniell gealled mot sir,\n",
      "\n",
      "VISTALO.\n",
      "I canyed,\n",
      " grom prighe for as!\n",
      "  BROSIFE. Dy the coomers thenggron beathes horwentr count, with a centerl; their tern,\n",
      "The neod Mon gee, saban; genom to oreple. Nere: a fromnings of the\n",
      "nighne's live op adrake for sheauid pangieer Peiour.\n",
      "eusisney right herens deeed dane ar\n",
      "8806/8806 [==============================] - 152s 17ms/step - loss: 2.0150\n",
      "Epoch 5/10\n",
      "8803/8806 [============================>.] - ETA: 0s - loss: 1.9758\n",
      "----- Generating text after Epoch: 4\n",
      "----- Generating with seed: \"\n",
      "  DUCHESS. Why then, I will. Farewell,\"\n",
      "\n",
      "  DUCHESS. Why then, I will. Farewell, my saus crumins indontdion,\n",
      "What I kned reas ar sas a lort. Hemmand, day, mysmory winl done on eventit; and if!\n",
      "  KICK  GUE PALRIUS. Get you doting And faith't feartn. He'ers’\n",
      "Ot no conturzablens me me,\n",
      "To prace, han say, hound histwird fentur for of coust,\n",
      "If Regien’s brookent and mangable, for watw\n",
      "\n",
      "Andank the dounhing onty; you he had not;\n",
      "I duve but mest in the excreust be you ane ablo\n",
      "8806/8806 [==============================] - 151s 17ms/step - loss: 1.9757\n",
      "Epoch 6/10\n",
      "8805/8806 [============================>.] - ETA: 0s - loss: 1.9413\n",
      "----- Generating text after Epoch: 5\n",
      "----- Generating with seed: \"s a grub. This Marcius is grown from man\"\n",
      "s a grub. This Marcius is grown from man, as he, for af any bangh,\n",
      ".   Horde, what Badath's farghce to ser.\n",
      "    Bat I have say and whelith, sors is may stalfy\n",
      "Have o's spbughansor of aeloouth to that\n",
      "    With conseing poor thic to be watry -reghter?\n",
      "And my to to specin. And I ware I dife’s pouet curtage\n",
      "To make vendeus of is glee.\n",
      "\n",
      "INRE\n",
      "That thick, herfalle himsandertachans man.\n",
      "    Enner wom’d. Eay they more; shooury you, nag\n",
      "8806/8806 [==============================] - 153s 17ms/step - loss: 1.9413\n",
      "Epoch 7/10\n",
      "8804/8806 [============================>.] - ETA: 0s - loss: 1.9127\n",
      "----- Generating text after Epoch: 6\n",
      "----- Generating with seed: \"ounds\n",
      "    That the old carlot once was \"\n",
      "ounds\n",
      "    That the old carlot once was with plead,\n",
      "Hrieds that weet dead une sow nete More;\n",
      "Teat you do was pair hanghtone! Which.\n",
      "  LUCIUS. It told te hames poroig, show yove ts um’nge?\n",
      "\n",
      "KINS.\n",
      "Ast if our feor have\n",
      "And toote wonth lipp inmy mearly, sir,\n",
      "Weishis awr’d oifer livors, of belif of youther\n",
      "Let me dot allobones on this incare a mojlot.\n",
      "  PADY. And a Phmghing to son.\n",
      "  PANUETO. I am woble, my shorl?\n",
      "               \n",
      "8806/8806 [==============================] - 159s 18ms/step - loss: 1.9128\n",
      "Epoch 8/10\n",
      "8804/8806 [============================>.] - ETA: 0s - loss: 1.8919\n",
      "----- Generating text after Epoch: 7\n",
      "----- Generating with seed: \"lack, that heaven should practise strata\"\n",
      "lack, that heaven should practise strataman up boubly no.\n",
      "I hine you he dants ontal go im monynf herw,\n",
      "Who shall keep takaina an latutiog with Gdvetitnsh did, and,\n",
      "    Thou brst than: I brings you would add the costius;\n",
      "Divewthou are foulowlls. Me oret, unbersugh heres,\n",
      "    Trave despilet of his ambress ambyol. Good not hill net conness’ at ing.\n",
      "For Ohme notull, indany of then sore Caunc\n",
      "    That voon mer anoy; if thy fither. Buc\n",
      "8806/8806 [==============================] - 156s 18ms/step - loss: 1.8919\n",
      "Epoch 9/10\n",
      "8805/8806 [============================>.] - ETA: 0s - loss: 1.8755\n",
      "----- Generating text after Epoch: 8\n",
      "----- Generating with seed: \"ome back. But I have spoke\n",
      "With one tha\"\n",
      "ome back. But I have spoke\n",
      "With one thank’s and as true spoking; and respare\n",
      "Ousameng as mumperl mind, then I knatertold\n",
      "    Th' youlgsbless hes ge me, toy did to rosh own laty.\n",
      "\n",
      "PERRNTNS.\n",
      "Have more:\n",
      "I wis me speen retustie to madenthinorigges:\n",
      "Would, shall be not cansss, he master his congind, or ame to thiek?\n",
      "    In you, and forfum one of I pracrons sould\n",
      "    Nor we truck a doughts now gon with ritom’s dees.\n",
      "We on thees and\n",
      "8806/8806 [==============================] - 158s 18ms/step - loss: 1.8754\n",
      "Epoch 10/10\n",
      "8804/8806 [============================>.] - ETA: 0s - loss: 1.8608\n",
      "----- Generating text after Epoch: 9\n",
      "----- Generating with seed: \"one sed it was an owle,\n",
      "The other he se\"\n",
      "one sed it was an owle,\n",
      "The other he sef: what passen at eyes sit.\n",
      "\n",
      "IIKIZINE.\n",
      "God my lort, Marty. (This wor of her kend;\n",
      "    As that in eddabl me infaster herk bloest,\n",
      "    Beard my pondern his resipit of of grom.\n",
      "\n",
      "LEENN.\n",
      "Sicl, and Grisces spone well, the praces of ourtent pack lives me,\n",
      "    Teathld. Maspishe gods I him I truess foo\n",
      "Terciclls’d you, what you the safetondss;\n",
      "Think my corsposs of me. Act is frim hen,\n",
      "    Ahe s\n",
      "8806/8806 [==============================] - 163s 19ms/step - loss: 1.8608\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1e01b5aa400>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model\n",
    "\n",
    "model.fit(x, y,\n",
    "          batch_size=32,\n",
    "          epochs=10,\n",
    "          callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zE4a4O7Bp5x1"
   },
   "source": [
    "# Resources and Stretch Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uT3UV3gap9H6"
   },
   "source": [
    "## Stretch goals:\n",
    "- Refine the training and generation of text to be able to ask for different genres/styles of Shakespearean text (e.g. plays versus sonnets)\n",
    "- Train a classification model that takes text and returns which work of Shakespeare it is most likely to be from\n",
    "- Make it more performant! Many possible routes here - lean on Keras, optimize the code, and/or use more resources (AWS, etc.)\n",
    "- Revisit the news example from class, and improve it - use categories or tags to refine the model/generation, or train a news classifier\n",
    "- Run on bigger, better data\n",
    "\n",
    "## Resources:\n",
    "- [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) - a seminal writeup demonstrating a simple but effective character-level NLP RNN\n",
    "- [Simple NumPy implementation of RNN](https://github.com/JY-Yoon/RNN-Implementation-using-NumPy/blob/master/RNN%20Implementation%20using%20NumPy.ipynb) - Python 3 version of the code from \"Unreasonable Effectiveness\"\n",
    "- [TensorFlow RNN Tutorial](https://github.com/tensorflow/models/tree/master/tutorials/rnn) - code for training a RNN on the Penn Tree Bank language dataset\n",
    "- [4 part tutorial on RNN](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/) - relates RNN to the vanishing gradient problem, and provides example implementation\n",
    "- [RNN training tips and tricks](https://github.com/karpathy/char-rnn#tips-and-tricks) - some rules of thumb for parameterizing and training your RNN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "U4-S2-NLP ",
   "language": "python",
   "name": "u4-s2-nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "nteract": {
   "version": "0.23.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
